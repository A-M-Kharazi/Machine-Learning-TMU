{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2e6174",
   "metadata": {},
   "source": [
    "# Classification and Feature Selection Using Regression\n",
    "<hr>\n",
    "\n",
    "In this notebook, we are trying to use linear regression with L1 and L2 loss function as well as OMP to extract important data within our dataset and classify our data.\n",
    "\n",
    "It is obvious that Classification using Regression is not an efficient method, however for teaching purposes it is a fun excerise to see how regression can be used to classify our data.\n",
    "\n",
    "One method to classify is the output from Regression is continus whereas in Classification we require distinct discrete values such as 0, 1, 2, etc; Therefore we need to somehow convert these. The relationship between data point (AKA sample) can also be represented using regression models. Therefore  samples with more weight ($|| W ||$) tends to have more influence in the original dataset ,therefore considered to be more important. Using this method can also lead to classification and summarization. \n",
    "<hr> \n",
    "    \n",
    "Everything required for this exercise is available at : \n",
    "   \n",
    " \n",
    "    \n",
    "   \n",
    "***GitHub***  : <a href = \"https://github.com/A-M-Kharazi/Machine-Learning-TMU.git\" > Main (class) repo </a> \n",
    "    &nbsp;&nbsp;&nbsp;\n",
    "    <a href = \"https://github.com/A-M-Kharazi/Machine-Learning-TMU/tree/main/Questions%20and%20Homeworks/HW-Series2\" > This Document page</a>\n",
    "    \n",
    "    \n",
    "***GoogleDrive*** : <a href = \"\" > Not available ATM </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466c862",
   "metadata": {},
   "source": [
    "#  Import libraries\n",
    "    \n",
    "<hr>\n",
    " \n",
    "    \n",
    "It is essential that we first load these libraries in our code so that it can work.\n",
    "    \n",
    "-  numpy, pandas, matplotlib are all necessary libraries to perform simple tasks on our data such as reading, creating dataframes, visualizing, mathematical operations, etc.\n",
    "   \n",
    "\n",
    "- pillow is used to read gif data (for video summarization task)\n",
    "\n",
    "    \n",
    "-  sklearn LinearRegression is used to create OLS regression (linear regression of common knowledge that is with L2 loss function)\n",
    "\n",
    "    \n",
    "-  ....\n",
    "    \n",
    " \n",
    "- sklearn OrthogonalMatchingPursuit is the OMP package\n",
    "\n",
    "\n",
    "- sklearn datasets is used to import ORL datasets.\n",
    "    \n",
    "If you don't have any/some of these installed, please <code> pip install </code> them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fe8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "import torch # Linear Regression using L1, L2 Loss Function\n",
    "from torch.autograd import Variable\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit as OMP\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724a165",
   "metadata": {},
   "source": [
    "#  Import data\n",
    "    \n",
    "<hr>\n",
    "    \n",
    "Data primary path is in a directory called  \"**Data** \" which is located in the main directory. In Data directory there is another directory calld H2 which contains the data that we'll be using for video part of our exercise.\n",
    "\n",
    "\n",
    "- Video Data : These data usually come with a .gif format. To obtain a 4D video matrix we propose the video_matrix function\n",
    "    This Function read the data using OpenCV (CV2) and extract its frames. Each frame contains an image of size X by Y, and each pixel contains an intensity of C (which can be gray scale, RGB , etc)\n",
    "    \n",
    "    Therefore video_matrix returns a 4D array such as [F, X, Y, C] where F is the frame, X and Y is the x and y location in the image (together they convey the location of the pixel within the image), and C is the color/ intensity of the pixel(x,y) where it can also be an array (like RGB).\n",
    "    \n",
    "    \n",
    "- ORL Data : ORL complete Description will be mentioned in the next section. To obtain this data we use the fetch method from sklearn.datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31052e",
   "metadata": {},
   "source": [
    "## Video Operation Functions\n",
    "\n",
    "These functions are used for data procesing and visualization of our Video data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9ff04",
   "metadata": {},
   "source": [
    "### Video_matrix Function\n",
    "\n",
    "This is a function to obtain matrix format of a video . That is a 4D matrix containing frame, location of pixels and their intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f116a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function convert the vide to a 4D matrix \n",
    "# First Dimension is for frames\n",
    "# Second is the image X \n",
    "# Third is the image Y\n",
    "# Last Dimension is the Color\n",
    "\n",
    "def video_matrix(video_path):\n",
    "    \n",
    "    # Read the Video \n",
    "    \n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    matrix = []\n",
    "    # Iterate through frames\n",
    "    while True:\n",
    "        # Capture a frame\n",
    "        flag, frame = video.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        \n",
    "        matrix.append(frame)\n",
    "    video.release()\n",
    "    return matrix   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf4b58",
   "metadata": {},
   "source": [
    "### show_video Function\n",
    "\n",
    "This function is used to present our video data. It requires a waitkey(default = 0) , closekey (default = q) , and video matrix (which can be obtained via previous function).\n",
    "\n",
    "To propely play the video, you need to **HOLD** the waitkey button on your keyboar. Normally video closes after it iterates through all its frames, however if you wish to close it before that happens, you can use the closekey button on your keyboard.\n",
    "\n",
    "\n",
    "Showing such videos can be tricky and if not careful, your notebook might crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function show a video using CV2 library\n",
    "# It can cause the notebook to crash if not careful\n",
    "# Input :   \n",
    "# Matrix # is the 4D matrix retrieved from video_matrix function \n",
    "# waitekey # is the waitkey button. To procced through different frames\n",
    "# To propely observe the video, you need to HOLD this button (or any other key)\n",
    "# closekey # is the key to close the video. Normally the video will close after\n",
    "# one whole iteration through its frames, however if tou wish to close it before\n",
    "# that happens, you can use this key. \n",
    "\n",
    "def show_video(matrix, waitkey = 0 , closekey = 'q', title = 'video'):\n",
    "    N = np.shape(matrix)[0]\n",
    "    print(f'Press {closekey} to exit and any other key to proceed through video')\n",
    "    for i in range(N):\n",
    "        cv2.imshow(title,matrix[i])\n",
    "        close = cv2.waitKey(waitkey)\n",
    "        if close == ord(closekey):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6f03f",
   "metadata": {},
   "source": [
    "### Video_matrix_to2D Function\n",
    "\n",
    "This Function converts each frame to a 1D array, therfore the 4D video matrix will be converted to a 2D video matrix. Conversion to a 1D array is done via the flatten() option within np. Using this method each sample with be a vector which we can use to fit our regression models.\n",
    "\n",
    "Considering original video matrix $\\in \\mathbb{R}^{F\\times X \\times Y \\times C}$, the new video matrix \n",
    "$\\in \\mathbb{R}^{F\\times XYC}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function flatten each frame so that each sample is a vector\n",
    "\n",
    "def video_matrix_to2D(video_matrix):\n",
    "    new_matrix = []\n",
    "    for frame in video_matrix:\n",
    "        # use flatten() to 1D the 3D data\n",
    "        sample = frame.flatten()\n",
    "        new_matrix.append(sample)\n",
    "    return new_matrix   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d252d",
   "metadata": {},
   "source": [
    "### Video_save Function\n",
    "\n",
    "This function takes the video_matrix and save it in the appropriate folder. Since images are RGB, be better save them as RGB.\n",
    "\n",
    "OpenCV uses the BGR format instead imageio uses the RGB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function saves the video matrix as a .gif image\n",
    "\n",
    "def video_save(video_matrix, name, path = '../Data/Summary/'):\n",
    "    \n",
    "    # convert frames to RGB\n",
    "    \n",
    "    new_frames = []\n",
    "    for frame in video_matrix:\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        new_frames.append(rgb_frame)\n",
    "        \n",
    "    imageio.mimsave(path + name, new_frames)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fba44",
   "metadata": {},
   "source": [
    "## Obtain Data\n",
    "\n",
    "Methods to obtain the data is already discussed in the previous sections. To obtain the ORL dataset, we use sklearn.dataset, as for video data types, the data will be the video matrix obtained from the video_matrix Function.\n",
    "\n",
    "\n",
    "**Download the videos from the link mentioned in the 'Data' folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos\n",
    "\n",
    "path = '../Data/H2'\n",
    "\n",
    "# Video 1 \n",
    "\n",
    "video1 = video_matrix(path + '/Data1.gif')\n",
    "\n",
    "# Video 2 \n",
    "\n",
    "video2 = video_matrix(path + '/Data2.gif')\n",
    "\n",
    "# Video 3 \n",
    "\n",
    "video3 = video_matrix(path + '/Data3.gif')\n",
    "\n",
    "# Video 4 \n",
    "\n",
    "video4 = video_matrix(path + '/Data4.webm')\n",
    "\n",
    "# Videos\n",
    "\n",
    "Video = {1: video1, 2: video2, 3: video3, 4:video4}\n",
    "\n",
    "# ORL\n",
    "\n",
    "ORL = sklearn.datasets.fetch_olivetti_faces()\n",
    "ORL_images = ORL['images']\n",
    "ORL_target = ORL['target']\n",
    "ORL_data = ORL['data'] # Is the flatten() version of each image\n",
    "ORL_desc = ORL['DESCR']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abef73",
   "metadata": {},
   "source": [
    "# Video Dataset\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this section we showcase and describe our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d23d42",
   "metadata": {},
   "source": [
    "## Video Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video 1\n",
    "\n",
    "print('Information about Video ',1)\n",
    "print('#################')\n",
    "print('It has ( ', np.shape(Video[1])[0],' ) frames')\n",
    "print('Each image has ( ', np.shape(Video[1])[1],' by', np.shape(Video[1])[2] ,' ) pixels')\n",
    "print('Each pixel has ( ', np.shape(Video[1])[3] ,' ) Colors')\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# Video 2\n",
    "\n",
    "print('Information about Video ',2)\n",
    "print('#################')\n",
    "print('It has ( ', np.shape(Video[2])[0],' ) frames')\n",
    "print('Each image has ( ', np.shape(Video[2])[1],' by', np.shape(Video[2])[2] ,' ) pixels')\n",
    "print('Each pixel has ( ', np.shape(Video[2])[3] ,' ) Colors')\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# Video 3\n",
    "\n",
    "print('Information about Video ',3)\n",
    "print('#################')\n",
    "print('It has ( ', np.shape(Video[3])[0],' ) frames')\n",
    "print('Each image has ( ', np.shape(Video[3])[1],' by', np.shape(Video[3])[2] ,' ) pixels')\n",
    "print('Each pixel has ( ', np.shape(Video[3])[3] ,' ) Colors')\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# Video 4\n",
    "\n",
    "print('Information about Video ',4)\n",
    "print('#################')\n",
    "print('It has ( ', np.shape(Video[4])[0],' ) frames')\n",
    "print('Each image has ( ', np.shape(Video[4])[1],' by', np.shape(Video[4])[2] ,' ) pixels')\n",
    "print('Each pixel has ( ', np.shape(Video[4])[3] ,' ) Colors')\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af4447",
   "metadata": {},
   "source": [
    "## Video showcase (Press 'q' to exit | Hold any other key to play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1005e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video 1\n",
    "\n",
    "show_video(Video[1], title = \"Data1\")\n",
    "\n",
    "# Video 2\n",
    "\n",
    "show_video(Video[2], title = \"Data2\")\n",
    "\n",
    "# Video 3\n",
    "\n",
    "show_video(Video[3], title = \"Data3\")\n",
    "\n",
    "\n",
    "# Video 4\n",
    "\n",
    "show_video(Video[4], title = \"Data4\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426e8b4",
   "metadata": {},
   "source": [
    "# ORL Dataset\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this section we showcase and describe our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3e207",
   "metadata": {},
   "source": [
    "## ORL Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ORL_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc5e97",
   "metadata": {},
   "source": [
    "## ORL Visualization (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_images:\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class \n",
    "    ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_target[indx-1]), fontsize = 20, color = 'red', fontweight='bold')\n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b52c03",
   "metadata": {},
   "source": [
    "# 0. Linear Regression (Using PyTorch)\n",
    "\n",
    "<hr>\n",
    "\n",
    "Since we are going to use OMP and Linear Regression (with L1 and L2 Loss Function) in this notebook, we need to define these methods. \n",
    "\n",
    "Sklearn contains OMP and Linear Regression (with L2 Loss Function). In case of Linear Regression (with L1 Loss Function), we need to build it from scratch. \n",
    "\n",
    "Therefore, we are going to use PyTorch to build our Linear Regression method (using L1 and L2 Loss Function). \n",
    "\n",
    "\n",
    "  $\\bullet$ LinearRegressionModel is a Linear Regression model with flexibility to use L1 and L2 loss function.\n",
    "  \n",
    "First you need to define data dimensions, then you can change the criterion_mode to 'l1' to perform L1 loss function optimization. Max_iter is set to 500 by default and represent the number of epochs. Optimizer learning rate is also set to 0.01.\n",
    "\n",
    "This model also contains a fit function where it fit the model using criterion (l1 or l2 loss function) and SGD optimizer. \n",
    "\n",
    "If log is set to true, it will print every epoch's result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, criterion_mode = 'l2', max_iter = 500, optimizer_lr = 0.01, _bias = False):\n",
    "        self.optimizer_lr = optimizer_lr\n",
    "        self.criterion_mode = criterion_mode\n",
    "        self.max_iter = max_iter\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim, bias = _bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, Y, log = True):\n",
    "        \n",
    "        if  self.criterion_mode == 'l1':\n",
    "            criterion = torch.nn.L1Loss()\n",
    "        else:\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            \n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = self.optimizer_lr)\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing\n",
    "            # x to the model\n",
    "    \n",
    "            Y_pred = self(X)\n",
    "        \n",
    "            # Compute and print loss\n",
    "        \n",
    "            loss = criterion(Y_pred, Y)\n",
    "        \n",
    "            # Zero gradients, perform a backward pass,\n",
    "            # and update the weights.\n",
    "            \n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # print loss per epoch (if log is set to True)\n",
    "            if log :\n",
    "                print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        return loss.item()\n",
    "    def coef(self):\n",
    "        \n",
    "        return self.linear.weight.detach().numpy().tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97fa7c",
   "metadata": {},
   "source": [
    "# 1. Video Summarization \n",
    "\n",
    "<hr>\n",
    "\n",
    "Our Objective is to use Linear Regression (L1 and L2 norm Loss function) and OMP to find the 10 percent most important frames withing each video data. \n",
    "\n",
    "\n",
    "Assume for each frame, our objective is to reconstruct that frame using remaining frames. Consider Video1 $\\in \\mathbb{R}^{188 \\times 128 \\times 220 \\times 3}$. By flattening each frame, each sample is $\\in \\mathbb{R}^{84480}$ ($84480 = 128\\times220\\times 3$), therefore video1 $\\in \\mathbb{R}^{188\\times 84480}$.\n",
    "\n",
    "\n",
    "Assume we have $X_1, X_2, \\dots, X_{188}$ as our samples. Our objective is to build $X_1$ using $X_2,\\dots, X_{188}$. To do that, we can create $X_1$ as a linear combination of $X_2, \\dots , X_{188}$. \n",
    "\n",
    "$$\n",
    "X_1 = W_0 + W_1(X_1 = 0 ) + W_2 X_2 + \\dots + W_{188} X_{188}\\\\\n",
    "X_1 = W^T  \\Phi(X_1)\n",
    "$$\n",
    "whereas :\n",
    "$$\n",
    "W^T = \\begin{bmatrix} W_0, W_1, \\dots, W_{188}\\end{bmatrix}\\hspace{1cm}\\text{and}\\hspace{1cm}\n",
    "\\Phi(X_1) = \\begin{bmatrix}\n",
    "1\\\\0\\\\X_2\\\\\\vdots\\\\X_{188}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can easily generalize this equation for $X_i$:\n",
    "\n",
    "$$\n",
    "X_i = W_0 + W_1X_1 + W_2X_2 + \\dots + W_{i-1}X_{i-1} + W_i(X_i  = 0) + W_{i+1} X_{i+1}+\\dots+ W_{188} X_{188}\\\\\n",
    "X_i = W^T  \\Phi(X_i)\\\\\n",
    "W^T = \\begin{bmatrix} W_0, W_1, \\dots, W_{188}\\end{bmatrix}\\hspace{1cm}\\text{and}\\hspace{1cm}\n",
    "\\Phi(X_i) = \\begin{bmatrix}\n",
    "1\\\\X_1\\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "X_{i-1}\\\\0\\\\X_{i+1}\n",
    "\\\\\n",
    "\\vdots\n",
    "\\\\X_{188}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The video1 itself is presented as $X$ that is $\\begin{bmatrix}X_1\\\\X_2\\\\\\vdots\\\\X_{188}\\end{bmatrix}$; The reconstructed video1 is presented as $\\hat X$ that is $\\begin{bmatrix}W^T\\Phi(X_1)\\\\W^T\\Phi(X_2)\\\\\\vdots\\\\W^T\\Phi(X_{188})\\end{bmatrix}$.\n",
    "\n",
    "\n",
    "To find the best model, we need to optimze $W$ in a way that $Loss(X, \\hat X )$ is minimized.\n",
    "\n",
    "\n",
    "Therefore, we can present a regression model (L1, L2 , or OMP) to find the best $W$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat X = \\Phi W\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Phi = \\begin{bmatrix}\n",
    "\\Phi(X_1)^T\\\\\n",
    "\\Phi(X_2)^T\\\\\n",
    "\\vdots\\\\\n",
    "\\Phi(X_{188})^T\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1,0,X_2,\\dots, X_{188}\\\\\n",
    "1, X_1, 0 , \\dots, X_{188}\\\\\n",
    "\\vdots\\\\\n",
    "1, X_1, X_2, \\dots, 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "W_0\\\\\n",
    "W_1\\\\\n",
    "\\vdots\\\\\n",
    "W_{188}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat X = \\begin{bmatrix}\n",
    "W_0 + W_1(0) + W_2 X_2 + \\dots + W_{188}X_188\\\\\n",
    "W_0 + W_1 X_1 + W_2(0) + \\dots + W_{188}X_188\\\\\n",
    "\\vdots\\\\\n",
    "W_0 + W_1 X_1 + W_2 X_2 + \\dots + W_{188}(0)\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\hat X_1\\\\\n",
    "\\hat X_2\\\\\n",
    "\\vdots\\\\\n",
    "\\hat X_{188}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Considering $W$, if we ignore $W_0$, each $W_i$ represent the contribution of sample $i$ in the reconstruction of video1.\n",
    "Which means that if $W_i \\geq W_j$ then Sample $i$ is more important than sample $j$. \n",
    "\n",
    "\n",
    "Using this technique, we can find our most important samples withing the data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278a7cd",
   "metadata": {},
   "source": [
    "## 1.0 Problems with implementation\n",
    "\n",
    "Considering this problem, Creating and operating over $\\Phi(X)$ is extremly slow and memory consuming , therefore you may encounter memory problems which will stop this notebook from working properly.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849503fb",
   "metadata": {},
   "source": [
    "### Memory Problem\n",
    "\n",
    "As mentioned before, memory problem (allocation, etc) is bound to happen due to extremly large size of $\\Phi(X)$ for our video data. $\\Phi(X)$ for data1 (video 1) contains 188 (#frames) rows where each row contains 15,966,720 (almost 16 million) columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee91d9",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "A solution that came to my mind was to create the model separately for each frame. That is create model $f_1$ to reconstruct frame $X_1$ by frames $X_2, \\dots , X_N$. This model will find the optimum $W$ corresponding to its data. Lets call the $W$ retrieved from $f_1$, $W^{(1)}$. Similarly we can produce other $W^{(i)}$s .\n",
    "\n",
    "\n",
    "$$\n",
    "\\Phi(X_1)W^{(1)} = \\hat X_1 \\hspace{1.5cm} \\dots \\hspace{1.5cm} \\Phi(X_N)W^{(N)} = \\hat X_N\n",
    "$$\n",
    "\n",
    "This means we have $N$ models, and we wish to find a relationship between these models. \n",
    "\n",
    "\n",
    "Our goal is to find the $20$ percent most important frames, that is if we have a single model; $||W_i||_2 \\in W$ shows the importance of sample $i$ in reconstruction, therefore if $||W_i||$ is bigger, then it is more important.\n",
    "\n",
    "\n",
    "How can we use the same idea to find important frames while having multiple models ?\n",
    "\n",
    "**Idea :**\n",
    "\n",
    "\n",
    "1. Generate $W^{(i)}$ for $i : 1, 2, \\dots, N$ (corresponding to model $f_i$ which reconstruct $X_i$ frame by other frames)\n",
    "\n",
    "2. Sort indices of  $W^{(i)}$ by their corresponding value (Descending)\n",
    "\n",
    "3. Generate a matrix $A$ such that it have $N$ rows ($W^{(i)}$ or sample $i$) and $N\\times \\frac{20}{100}$ coloumns ($20$ percent most important frames.)\n",
    "\n",
    "    - To understand this, each $W^{(i)}$ has $N$ elements. Once sorted, we wish to choose only $20$ percent of them (location).\n",
    "    \n",
    "4. For each $a_{i,j} \\in A$, count the number of sample $i$ being placed at location $j$.\n",
    "\n",
    "    - For example, $a_{1,1}$ means how many times sample $1$ (or $W_1$) has been placed at location $1$ in the sorted indices of $W^{(i)}$s for every $i$\n",
    "\n",
    "\n",
    "5. Iterate through $A$, find the biggest counter (for example $a_{i,j}$) and save $i$ as one of the important samples. Then remove row $i$ from $A$ so that sample $i$ cannot be choosen again. Loop this process until the number of important samples have been satisfied.\n",
    "\n",
    "\n",
    "6. Sort the important frames and play them.\n",
    "\n",
    "\n",
    "\n",
    "**In Summary** :\n",
    "\n",
    "Generate $W$ for each model and sort their indices by values. Find the top $20$ percent of each $W$ and generate a matrix called $A$ so that each row of $A$ counts the number of sample $i$ has been observed at position $j$. For example $a_{3,4}$ means how many times sample $3$ has been at position $4$ in all $W$s. Once $A$ has been constructed, loop through $A$ and find the biggest counter; save its sample in another list (important_frames), and remove the row which had the biggest counter so it can't be choosen again. Once important_frames's list is generated (which means its length is satisfied), sort them by order of frames (not values) and play them as a video. \n",
    "\n",
    "Sorting them by frames at the end of this process, causes the summary video to start  with important frames which start at the begining of the original video and end with those that ends the original video. (the timeline of the video is saved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967350d",
   "metadata": {},
   "source": [
    "## 1.1 Data 1 \n",
    "\n",
    "Using the solution mentioned above to fix the memory problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9eac4",
   "metadata": {},
   "source": [
    "### 1.1.1 Linear Regression (L2 Loss Function)\n",
    "\n",
    "Linear Regression using PyTorch may fail due to inappropriate optimizer and learning rate, Therefore only for this case, we use sklearn LinearRegression model.\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Pytorch may fail due to inappropriate optimizer and learning rate\n",
    "# Therefore just for this case, we use sklearn LinearRegression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as temporary_l2model\n",
    "\n",
    "video2d_1 = np.array(video_matrix_to2D(Video[1]))\n",
    "\n",
    "end = round(np.shape(video2d_1)[0] * (20/100))\n",
    "\n",
    "A_l2_data1 = np.zeros((video2d_1.shape[0], end))\n",
    "\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_1):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_1):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = temporary_l2model()\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l2_data1[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7038e",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list1_l2 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l2_data1.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l2_data1.shape[0]):\n",
    "        if row not in important_frames_list1_l2:\n",
    "            if A_l2_data1[row][column] >= temp_max:\n",
    "                temp_max = A_l2_data1[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list1_l2.append(max_index)\n",
    "\n",
    "sorted_important_frames_list1_l2 = np.sort(important_frames_list1_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1184a",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video1_summary_frames_l2 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list1_l2:\n",
    "    video1_summary_frames_l2.append(Video[1][frame_index])\n",
    "    \n",
    "show_video(video1_summary_frames_l2, title= 'Data1 Summary by L2 ')\n",
    "video_save(video1_summary_frames_l2, name='Data1Summary_l2.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a93c9d",
   "metadata": {},
   "source": [
    "### 1.1.2 Linear Regression (L1 Loss Function)\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2d_1 = np.array(video_matrix_to2D(Video[1]))\n",
    "end = round(np.shape(video2d_1)[0] * (20/100))\n",
    "\n",
    "A_l1_data1 = np.zeros((video2d_1.shape[0], end))\n",
    "\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_1):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_1):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model and variables\n",
    "    \n",
    "    x = Variable(torch.Tensor(x))\n",
    "    y = Variable(torch.Tensor(y))\n",
    "    \n",
    "    model = LinearRegressionModel(x.shape[1],y.shape[1], criterion_mode='l1', optimizer_lr= 0.000001, max_iter = 100)\n",
    "    \n",
    "    # Print a test log <start>\n",
    "    \n",
    "    print(f'Currently building sample {index1}')\n",
    "    \n",
    "    # Print a test log <end>\n",
    "    \n",
    "    trash = model.fit(x,y, log=False)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = np.array(model.coef()).flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l1_data1[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb00b1",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list1_l1 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l1_data1.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l1_data1.shape[0]):\n",
    "        if row not in important_frames_list1_l1:\n",
    "            if A_l1_data1[row][column] >= temp_max:\n",
    "                temp_max = A_l1_data1[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list1_l1.append(max_index)\n",
    "\n",
    "sorted_important_frames_list1_l1 = np.sort(important_frames_list1_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cb724",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "video1_summary_frames_l1 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list1_l1:\n",
    "    video1_summary_frames_l1.append(Video[1][frame_index])\n",
    "    \n",
    "show_video(video1_summary_frames_l1, title='Data1 Summary by L1 ')\n",
    "video_save(video1_summary_frames_l1, name='Data1Summary_l1.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d8cba",
   "metadata": {},
   "source": [
    "### 1.1.3 OMP\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2d_1 = np.array(video_matrix_to2D(Video[1]))\n",
    "end = round(np.shape(video2d_1)[0] * (20/100))\n",
    "\n",
    "A_omp_data1 = np.zeros((video2d_1.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_1):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_1):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = OMP(fit_intercept=False, normalize=False, n_nonzero_coefs= 5)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_omp_data1[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7766417",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list1_omp = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_omp_data1.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_omp_data1.shape[0]):\n",
    "        if row not in important_frames_list1_omp:\n",
    "            if A_omp_data1[row][column] >= temp_max:\n",
    "                temp_max = A_omp_data1[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list1_omp.append(max_index)\n",
    "\n",
    "sorted_important_frames_list1_omp = np.sort(important_frames_list1_omp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f33f1",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video1_summary_frames_omp = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list1_omp:\n",
    "    video1_summary_frames_omp.append(Video[1][frame_index])\n",
    "    \n",
    "show_video(video1_summary_frames_omp, title= 'Data1 Summary by OMP ')\n",
    "video_save(video1_summary_frames_omp, name='Data1Summary_omp.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f494e1e",
   "metadata": {},
   "source": [
    "## 1.2 Data 2\n",
    "\n",
    "Using the solution mentioned above to fix the memory problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccd2cc",
   "metadata": {},
   "source": [
    "### 1.2.1 Linear Regression (L2 Loss Function)\n",
    "\n",
    "Linear Regression using PyTorch may fail due to inappropriate optimizer and learning rate, Therefore only for this case, we use sklearn LinearRegression model.\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8830ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Pytorch may fail due to inappropriate optimizer and learning rate\n",
    "# Therefore just for this case, we use sklearn LinearRegression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as temporary_l2model\n",
    "\n",
    "video2d_2 = np.array(video_matrix_to2D(Video[2]))\n",
    "\n",
    "end = round(np.shape(video2d_2)[0] * (20/100))\n",
    "\n",
    "A_l2_data2 = np.zeros((video2d_2.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_2):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_2):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = temporary_l2model()\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l2_data2[i][j] += 1\n",
    "        \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7462a7",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8110dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list2_l2 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l2_data2.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l2_data2.shape[0]):\n",
    "        if row not in important_frames_list2_l2:\n",
    "            if A_l2_data2[row][column] >= temp_max:\n",
    "                temp_max = A_l2_data2[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list2_l2.append(max_index)\n",
    "\n",
    "sorted_important_frames_list2_l2 = np.sort(important_frames_list2_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c831ea",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d96d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2_summary_frames_l2 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list2_l2:\n",
    "    video2_summary_frames_l2.append(Video[2][frame_index])\n",
    "    \n",
    "show_video(video2_summary_frames_l2, title= 'Data2 Summary by L2 ')\n",
    "video_save(video2_summary_frames_l2, name='Data2Summary_l2.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efbb27",
   "metadata": {},
   "source": [
    "### 1.2.2 Linear Regression (L1 Loss Function)\n",
    "\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970b5a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video2d_2 = np.array(video_matrix_to2D(Video[2]))\n",
    "\n",
    "end = round(np.shape(video2d_2)[0] * (20/100))\n",
    "\n",
    "A_l1_data2 = np.zeros((video2d_2.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_2):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_2):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model and variables\n",
    "    \n",
    "    x = Variable(torch.Tensor(x))\n",
    "    y = Variable(torch.Tensor(y))\n",
    "    \n",
    "    model = LinearRegressionModel(x.shape[1],y.shape[1], criterion_mode='l1', optimizer_lr= 0.000001, max_iter = 100)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    # Print a test log <start>\n",
    "    \n",
    "    print(f'Currently building sample {index1}')\n",
    "    \n",
    "    # Print a test log <end>\n",
    "    \n",
    "    trash = model.fit(x,y, log = True)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = np.array(model.coef()).flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l1_data2[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43146426",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list2_l1 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l1_data2.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l1_data2.shape[0]):\n",
    "        if row not in important_frames_list2_l1:\n",
    "            if A_l1_data2[row][column] >= temp_max:\n",
    "                temp_max = A_l1_data2[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list2_l1.append(max_index)\n",
    "\n",
    "sorted_important_frames_list2_l1 = np.sort(important_frames_list2_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15157c11",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c117b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2_summary_frames_l1 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list2_l1:\n",
    "    video2_summary_frames_l1.append(Video[2][frame_index])\n",
    "    \n",
    "show_video(video2_summary_frames_l1, title= 'Data2 Summary by L1 ')\n",
    "video_save(video2_summary_frames_l1, name='Data2Summary_l1.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0ca62",
   "metadata": {},
   "source": [
    "### 1.2.3 OMP\n",
    "\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2d_2 = np.array(video_matrix_to2D(Video[2]))\n",
    "\n",
    "end = round(np.shape(video2d_2)[0] * (20/100))\n",
    "\n",
    "A_omp_data2 = np.zeros((video2d_2.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_2):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_2):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = OMP(fit_intercept=False, normalize=False, n_nonzero_coefs= 5)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_omp_data2[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d70b8",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list2_omp = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_omp_data2.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_omp_data2.shape[0]):\n",
    "        if row not in important_frames_list2_omp:\n",
    "            if A_omp_data2[row][column] >= temp_max:\n",
    "                temp_max = A_omp_data2[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list2_omp.append(max_index)\n",
    "\n",
    "sorted_important_frames_list2_omp = np.sort(important_frames_list2_omp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9746716",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c83326",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2_summary_frames_omp = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list2_omp:\n",
    "    video2_summary_frames_omp.append(Video[2][frame_index])\n",
    "    \n",
    "show_video(video2_summary_frames_omp, title = 'Data2 Summary by OMP ')\n",
    "video_save(video2_summary_frames_omp, name='Data2Summary_omp.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901ce9d",
   "metadata": {},
   "source": [
    "## 1.3 Data 3\n",
    "\n",
    "Using the solution mentioned above to fix the memory problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978491fa",
   "metadata": {},
   "source": [
    "### 1.3.1 Linear Regression (L2 Loss Function)\n",
    "\n",
    "Linear Regression using PyTorch may fail due to inappropriate optimizer and learning rate, Therefore only for this case, we use sklearn LinearRegression model.\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacf1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Pytorch may fail due to inappropriate optimizer and learning rate\n",
    "# Therefore just for this case, we use sklearn LinearRegression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as temporary_l2model\n",
    "\n",
    "video2d_3 = np.array(video_matrix_to2D(Video[3]))\n",
    "\n",
    "end = round(np.shape(video2d_3)[0] * (20/100))\n",
    "\n",
    "A_l2_data3 = np.zeros((video2d_3.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_3):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_3):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = temporary_l2model()\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l2_data3[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568e7da",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839dddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list3_l2 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l2_data3.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l2_data3.shape[0]):\n",
    "        if row not in important_frames_list3_l2:\n",
    "            if A_l2_data3[row][column] >= temp_max:\n",
    "                temp_max = A_l2_data3[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list3_l2.append(max_index)\n",
    "\n",
    "sorted_important_frames_list3_l2 = np.sort(important_frames_list3_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0244c27",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video3_summary_frames_l2 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list3_l2:\n",
    "    video3_summary_frames_l2.append(Video[3][frame_index])\n",
    "    \n",
    "show_video(video3_summary_frames_l2, title = 'Data3 Summary by L2 ')\n",
    "video_save(video3_summary_frames_l2, name='Data3Summary_l2.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5898c26",
   "metadata": {},
   "source": [
    "### 1.3.2 Linear Regression (L1 Loss Function)\n",
    "\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766023a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video2d_3 = np.array(video_matrix_to2D(Video[3]))\n",
    "\n",
    "end = round(np.shape(video2d_3)[0] * (20/100))\n",
    "\n",
    "A_l1_data3 = np.zeros((video2d_3.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_3):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_3):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model and variables\n",
    "    \n",
    "    x = Variable(torch.Tensor(x))\n",
    "    y = Variable(torch.Tensor(y))\n",
    "    \n",
    "    model = LinearRegressionModel(x.shape[1],y.shape[1], criterion_mode='l1', optimizer_lr= 0.000001, max_iter = 100)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    # Print a test log <start>\n",
    "    \n",
    "    print(f'Currently building sample {index1}')\n",
    "    \n",
    "    # Print a test log <end>\n",
    "    \n",
    "    trash = model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = np.array(model.coef()).flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l1_data3[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405ec08",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf24fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list3_l1 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l1_data3.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l1_data3.shape[0]):\n",
    "        if row not in important_frames_list3_l1:\n",
    "            if A_l1_data3[row][column] >= temp_max:\n",
    "                temp_max = A_l1_data3[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list3_l1.append(max_index)\n",
    "\n",
    "sorted_important_frames_list3_l1 = np.sort(important_frames_list3_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e354d",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video3_summary_frames_l1 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list3_l1:\n",
    "    video3_summary_frames_l1.append(Video[3][frame_index])\n",
    "    \n",
    "show_video(video3_summary_frames_l1, title = 'Data3 Summary by L1 ')\n",
    "video_save(video3_summary_frames_l1, name='Data3Summary_l1.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca87385",
   "metadata": {},
   "source": [
    "### 1.3.3 OMP\n",
    "\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe51b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2d_3 = np.array(video_matrix_to2D(Video[3]))\n",
    "\n",
    "end = round(np.shape(video2d_3)[0] * (20/100))\n",
    "\n",
    "A_omp_data3 = np.zeros((video2d_3.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_3):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_3):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = OMP(fit_intercept=False, normalize=False, n_nonzero_coefs= 5)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_omp_data3[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660013f",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af083888",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list3_omp = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_omp_data3.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_omp_data3.shape[0]):\n",
    "        if row not in important_frames_list3_omp:\n",
    "            if A_omp_data3[row][column] >= temp_max:\n",
    "                temp_max = A_omp_data3[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list3_omp.append(max_index)\n",
    "\n",
    "sorted_important_frames_list3_omp = np.sort(important_frames_list3_omp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd7124",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video3_summary_frames_omp = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list3_omp:\n",
    "    video3_summary_frames_omp.append(Video[3][frame_index])\n",
    "    \n",
    "show_video(video3_summary_frames_omp, title = 'Data3 Summary by OMP ')\n",
    "video_save(video3_summary_frames_omp, name='Data3Summary_omp.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2d63a",
   "metadata": {},
   "source": [
    "## 1.4 Data 4\n",
    "\n",
    "Using the solution mentioned above to fix the memory problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c009b7",
   "metadata": {},
   "source": [
    "### 1.4.1 Linear Regression (L2 Loss Function)\n",
    "\n",
    "Linear Regression using PyTorch may fail due to inappropriate optimizer and learning rate, Therefore only for this case, we use sklearn LinearRegression model.\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e059d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Pytorch may fail due to inappropriate optimizer and learning rate\n",
    "# Therefore just for this case, we use sklearn LinearRegression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as temporary_l2model\n",
    "\n",
    "video2d_4 = np.array(video_matrix_to2D(Video[4]))\n",
    "\n",
    "end = round(np.shape(video2d_4)[0] * (20/100))\n",
    "\n",
    "A_l2_data4 = np.zeros((video2d_4.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_4):\n",
    "    \n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_4):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = temporary_l2model()\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l2_data4[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7896e5",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list4_l2 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l2_data4.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l2_data4.shape[0]):\n",
    "        if row not in important_frames_list4_l2:\n",
    "            if A_l2_data4[row][column] >= temp_max:\n",
    "                temp_max = A_l2_data4[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list4_l2.append(max_index)\n",
    "\n",
    "sorted_important_frames_list4_l2 = np.sort(important_frames_list4_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a6c52",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "video4_summary_frames_l2 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list4_l2:\n",
    "    video4_summary_frames_l2.append(Video[4][frame_index])\n",
    "    \n",
    "show_video(video4_summary_frames_l2, title = 'Data4 Summary by L2 ')\n",
    "video_save(video4_summary_frames_l2, name='Data4Summary_l2.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea02b4",
   "metadata": {},
   "source": [
    "### 1.4.2 Linear Regression (L1 Loss Function)\n",
    "\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2d_4 = np.array(video_matrix_to2D(Video[4]))\n",
    "\n",
    "end = round(np.shape(video2d_4)[0] * (20/100))\n",
    "\n",
    "A_l1_data4 = np.zeros((video2d_4.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_4):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_4):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model and variables\n",
    "    \n",
    "    x = Variable(torch.Tensor(x))\n",
    "    y = Variable(torch.Tensor(y))\n",
    "    \n",
    "    model = LinearRegressionModel(x.shape[1],y.shape[1], criterion_mode='l1', optimizer_lr= 0.000001, max_iter = 100)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    # Print a test log <start>\n",
    "    \n",
    "    print(f'Currently building sample {index1}')\n",
    "    \n",
    "    # Print a test log <end>\n",
    "    \n",
    "    trash = model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = np.array(model.coef()).flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_l1_data4[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24db31",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65073088",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list4_l1 = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_l1_data4.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_l1_data4.shape[0]):\n",
    "        if row not in important_frames_list4_l1:\n",
    "            if A_l1_data4[row][column] >= temp_max:\n",
    "                temp_max = A_l1_data4[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list4_l1.append(max_index)\n",
    "\n",
    "sorted_important_frames_list4_l1 = np.sort(important_frames_list4_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba18692",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "video4_summary_frames_l1 = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list4_l1:\n",
    "    video4_summary_frames_l1.append(Video[4][frame_index])\n",
    "    \n",
    "show_video(video4_summary_frames_l1, title = 'Data4 Summary by L1 ')\n",
    "video_save(video4_summary_frames_l1, name='Data4Summary_l1.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bb86b",
   "metadata": {},
   "source": [
    "### 1.4.3 OMP\n",
    "\n",
    "\n",
    "**This will take some time , please be patient ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c592e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2d_4 = np.array(video_matrix_to2D(Video[4]))\n",
    "\n",
    "end = round(np.shape(video2d_4)[0] * (20/100))\n",
    "\n",
    "A_omp_data4 = np.zeros((video2d_4.shape[0], end))\n",
    "\n",
    "for (index1, sample1) in enumerate(video2d_4):\n",
    "    \n",
    "    phi_xi = []\n",
    "    \n",
    "    # generate X and Y\n",
    "    \n",
    "    for (index2,sample2) in enumerate(video2d_4):\n",
    "        \n",
    "        if index2!= index1:\n",
    "            phi_xi.append(sample2)\n",
    "    \n",
    "    x = np.array(phi_xi).T\n",
    "    y = sample1.reshape(-1,1)\n",
    "    \n",
    "    # create the model\n",
    "    \n",
    "    model = OMP(fit_intercept=False, normalize=False, n_nonzero_coefs= 2)\n",
    "\n",
    "    # fit the model\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # calculate W\n",
    "    \n",
    "    Wi = model.coef_.flatten()\n",
    "    # +1 for W_i of sample i which will be set to zero later.\n",
    "    indices = np.arange(Wi.shape[0]+1)\n",
    "    \n",
    "    # adjust Wi to include W_i for sample i with 0 value\n",
    "    Wi = Wi.tolist()\n",
    "    Wi.insert(index1, 0)\n",
    "    Wi = np.array(Wi)\n",
    "    \n",
    "    # Prepare to sort (absolute value of W_i for each Wi)\n",
    "    \n",
    "    W_dataframe = pd.DataFrame({'index':indices, 'W_i': np.abs(Wi)})\n",
    "    W_dataframe = W_dataframe.sort_values(by='W_i', ascending=False)\n",
    "    \n",
    "    # Update a_{ij} based on the sorted W_dataframe\n",
    "    \n",
    "    for j in range(end):\n",
    "        i = np.array(W_dataframe['index'])[j]\n",
    "        # Increase the counter by 1\n",
    "        A_omp_data4[i][j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42745d04",
   "metadata": {},
   "source": [
    "#### Find the important frames\n",
    "\n",
    "After building the $A$ matrix, we can loop through it and find the $20$ percent most important frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_frames_list4_omp = []\n",
    "\n",
    "# Find most 20 percent most important frames.\n",
    "\n",
    "for column in range(A_omp_data4.shape[1]):\n",
    "    temp_max = 0\n",
    "    max_index = 0\n",
    "    for row in range(A_omp_data4.shape[0]):\n",
    "        if row not in important_frames_list4_omp:\n",
    "            if A_omp_data4[row][column] >= temp_max:\n",
    "                temp_max = A_omp_data4[row][column]\n",
    "                max_index = row\n",
    "                \n",
    "    important_frames_list4_omp.append(max_index)\n",
    "\n",
    "sorted_important_frames_list4_omp = np.sort(important_frames_list4_omp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb10687",
   "metadata": {},
   "source": [
    "#### Generate the new video and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d592c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "video4_summary_frames_omp = []\n",
    "\n",
    "for frame_index in sorted_important_frames_list4_omp:\n",
    "    video4_summary_frames_omp.append(Video[4][frame_index])\n",
    "    \n",
    "show_video(video4_summary_frames_omp, title = 'Data4 Summary by OMP ')\n",
    "video_save(video4_summary_frames_omp, name='Data4Summary_omp.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd07a8",
   "metadata": {},
   "source": [
    "# 2. ORL Classification\n",
    "\n",
    "<hr>\n",
    "\n",
    "Our Objective is to use Linear Regression  (L1 and L2 norm Loss function) and OMP to Classifiy ORL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b74ea",
   "metadata": {},
   "source": [
    "## 2.1 Method 1\n",
    "\n",
    "In this method, we use onehotencoder approach towards classification. Three simple Steps:\n",
    "\n",
    "1. Convert categorical output to one hot vector\n",
    "\n",
    "\n",
    "2. Create your model by X ($\\in \\mathbb{R}^{N\\times d}$) and Y ($\\in \\mathbb{R}^{N\\times D}$)\n",
    "\n",
    "\n",
    "3. Estimate categorical $\\hat Y$ using vector $\\hat Y$ \n",
    "    - that is to find the element which contains the maximum value and call its index as the categorical value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e1d46",
   "metadata": {},
   "source": [
    "### 2.1.1 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba371f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORL_train_x, ORL_test_x, ORL_train_y, ORL_test_y= train_test_split(ORL_data, ORL_target, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f081cf7",
   "metadata": {},
   "source": [
    "### 2.1.2 Vectorize labels  (one hot encoding)\n",
    "\n",
    "Using this approach, we are going to vectorize each labels.\n",
    "\n",
    "Apprach :\n",
    "\n",
    "1. $Y$ currently belongs to $\\mathbb{R}^1$ and can contain a distinct value from set $\\left\\{0,1,2,\\dots,39\\right\\}$\n",
    "    for example $Y$ can be 0 or 10 for sample $X_i$\n",
    "    \n",
    "    \n",
    "2. Vectorize $Y$ so that it belongs to $\\mathbb{R}^K$ (In our case $K$, number of classes,  is 40 )\n",
    "\n",
    "\n",
    "3. For each $Y$, set the $i$th element of $Y$ to 1 and the rest to 0 ( $i$ is the previous distinct value of $Y$ for sample $X_j$)\n",
    "\n",
    "\n",
    "For example :\n",
    "\n",
    "Conside $Y$ to be 3 for $X_i$, therefore vectorized $Y$ is $[0,0,0,1,0,\\dots, 0]$  where the $Y[3]$ element of $Y$\n",
    " is set to 1. Since $Y$ is an array, the counter for its elements starts with 0, thus the fourth element is set to 1. \n",
    " \n",
    " \n",
    "Consider $Y$ to be $\\left\\{1,2,3\\right\\}$ and $Y_i$ for $X_i$ is 2. Vectorized vestion of $Y_i$ is $[0,1,0]$ where the second element is set to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdef422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize each target (one hot encoder)\n",
    "\n",
    "K = np.shape(np.unique(ORL_target))[0]\n",
    "\n",
    "def encoder(target, _K = K):\n",
    "    target_vec = []\n",
    "    for _target in target:\n",
    "        vector = np.zeros(_K)\n",
    "        vector[_target] = 1\n",
    "        target_vec.append(vector)\n",
    "    return target_vec\n",
    "\n",
    "        \n",
    "# Vectorize Train Y \n",
    "\n",
    "ORL_train_y_vec = encoder(ORL_train_y)\n",
    "\n",
    "# Vectorize Test Y\n",
    "\n",
    "ORL_test_y_vec = encoder(ORL_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e84f59",
   "metadata": {},
   "source": [
    "### 2.1.3 Regression\n",
    "\n",
    "Using Linear Regression with (L1 and L2 loss function) and OMP, we will create 3 models to estimate $Y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38373f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimX = np.shape(ORL_train_x)[1]\n",
    "dimY = np.shape(ORL_train_y_vec)[1]\n",
    "\n",
    "trainX = Variable(torch.Tensor(ORL_train_x))\n",
    "trainY = Variable(torch.Tensor(ORL_train_y_vec))\n",
    "testX  = Variable(torch.Tensor(ORL_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7069e6b",
   "metadata": {},
   "source": [
    "#### 2.1.3.1 Linear Regression (L2 loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "ORL_model_l2 = LinearRegressionModel(dimX, dimY, criterion_mode= 'l2', max_iter= 1000, optimizer_lr= 0.01)\n",
    "ORL_model_l2.fit(trainX,trainY, True)\n",
    "\n",
    "# Generate Results\n",
    "\n",
    "ORL_model_l2_estimate_train_vec = ORL_model_l2(trainX)\n",
    "ORL_model_l2_estimate_test_vec  = ORL_model_l2(testX)\n",
    "\n",
    "# Convert Tensor to array\n",
    "\n",
    "ORL_model_l2_estimate_train_vec = ORL_model_l2_estimate_train_vec.detach().numpy().tolist()\n",
    "ORL_model_l2_estimate_test_vec  = ORL_model_l2_estimate_test_vec.detach().numpy().tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4edf4",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Linear Regression (L1 loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "ORL_model_l1 = LinearRegressionModel(dimX, dimY, criterion_mode= 'l1', max_iter= 1000, optimizer_lr= 0.01)\n",
    "ORL_model_l1.fit(trainX,trainY, True)\n",
    "\n",
    "# Generate Results\n",
    "\n",
    "ORL_model_l1_estimate_train_vec = ORL_model_l1(trainX)\n",
    "ORL_model_l1_estimate_test_vec  = ORL_model_l1(testX)\n",
    "\n",
    "# Convert Tensor to array\n",
    "\n",
    "ORL_model_l1_estimate_train_vec = ORL_model_l1_estimate_train_vec.detach().numpy().tolist()\n",
    "ORL_model_l1_estimate_test_vec  = ORL_model_l1_estimate_test_vec.detach().numpy().tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3b11d",
   "metadata": {},
   "source": [
    "#### 2.1.3.3 OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d06da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "ORL_model_omp = OMP(n_nonzero_coefs=5,fit_intercept=False, normalize= False)\n",
    "ORL_model_omp.fit(ORL_train_x,ORL_train_y_vec)\n",
    "\n",
    "# Generate Results\n",
    "\n",
    "ORL_model_omp_estimate_train_vec = ORL_model_omp.predict(ORL_train_x)\n",
    "ORL_model_omp_estimate_test_vec  = ORL_model_omp.predict(ORL_test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a7dab",
   "metadata": {},
   "source": [
    "### 2.1.4 Classify by Regression results\n",
    "\n",
    "Using The results of Regression models. Each model will predict a $\\mathbb{R}^{K=40}$ vector. Previously each vector contained only 0 or 1(at the $i$th element), however by regression we don't have such luxury.\n",
    "\n",
    "Previously we defined that $i$th element of each vector will decide its class. 1 is bigger than 0, therefore it might be a good first step towards classification to choose the biggest element as classification measure.  \n",
    "\n",
    "\n",
    "Approach :\n",
    "\n",
    "1. Obtain Predicted Values of target using Regression models\n",
    "\n",
    "\n",
    "2. For each target ($Y_i \\in \\mathbb{R}^40$): Find The biggest Element $k$ such as $\\forall j\\in \\left\\{0,1,\\dots,39\\right\\} :  Y_i[k] \\geq Y_i[j]$\n",
    "\n",
    "\n",
    "3. choose $k$ as the class of $Y_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d008d",
   "metadata": {},
   "source": [
    "#### 2.1.4.1 Linear Regression (L2 loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using predicted vectors\n",
    "\n",
    "# Train \n",
    "\n",
    "ORL_model_l2_estimate_train = []\n",
    "for target in np.abs(ORL_model_l2_estimate_train_vec):\n",
    "    k = np.argmax(target)\n",
    "    ORL_model_l2_estimate_train.append(k)\n",
    "    \n",
    "# Test\n",
    "\n",
    "ORL_model_l2_estimate_test = []\n",
    "for target in np.abs(ORL_model_l2_estimate_test_vec):\n",
    "    k = np.argmax(target)\n",
    "    ORL_model_l2_estimate_test.append(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f20724",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 Linear Regression (L1 loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3593c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using predicted vectors\n",
    "\n",
    "# Train \n",
    "\n",
    "ORL_model_l1_estimate_train = []\n",
    "for target in np.abs(ORL_model_l1_estimate_train_vec):\n",
    "    k = np.argmax(target)\n",
    "    ORL_model_l1_estimate_train.append(k)\n",
    "    \n",
    "# Test\n",
    "\n",
    "ORL_model_l1_estimate_test = []\n",
    "for target in np.abs(ORL_model_l1_estimate_test_vec):\n",
    "    k = np.argmax(target)\n",
    "    ORL_model_l1_estimate_test.append(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c4d93",
   "metadata": {},
   "source": [
    "#### 2.1.4.3 OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0233d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using predicted vectors\n",
    "\n",
    "# Train \n",
    "\n",
    "ORL_model_omp_estimate_train = []\n",
    "for target in np.abs(ORL_model_omp_estimate_train_vec):\n",
    "    k = np.argmax(target)\n",
    "    ORL_model_omp_estimate_train.append(k)\n",
    "    \n",
    "# Test\n",
    "\n",
    "ORL_model_omp_estimate_test = []\n",
    "for target in np.abs(ORL_model_omp_estimate_test_vec):\n",
    "    k = np.argmax(target)\n",
    "    ORL_model_omp_estimate_test.append(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5109137",
   "metadata": {},
   "source": [
    "### 2.1.5 Visualize Results\n",
    "\n",
    "\n",
    "Since it is a classification task, RMSE doesn't work. New metric to evaluate our classification model is required. \n",
    "\n",
    "We use F1 and accuracy score to evaluate our models. The F1 and accuracy score is a number between 0(worst) and 1(best)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef87bc",
   "metadata": {},
   "source": [
    "#### 2.1.5.1 Linear Regression (L2 loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf5d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# F1 score on train and test \n",
    "\n",
    "ORL_model_l2_f1_train = f1_score(ORL_train_y, ORL_model_l2_estimate_train, average='macro')\n",
    "ORL_model_l2_f1_test  = f1_score(ORL_test_y, ORL_model_l2_estimate_test, average='macro')\n",
    "\n",
    "# Accuracy score on test\n",
    "\n",
    "ORL_model_l2_accuracy_train = accuracy_score(ORL_train_y, ORL_model_l2_estimate_train)\n",
    "ORL_model_l2_accuracy_test  = accuracy_score(ORL_test_y, ORL_model_l2_estimate_test)\n",
    "\n",
    "# Print Result\n",
    "\n",
    "print(f'f1 score for training dataset using Linear Regression and L2 loss function : {ORL_model_l2_f1_train}')\n",
    "print(f'f1 score for testing dataset using Linear Regression and L2 loss function : {ORL_model_l2_f1_test}')\n",
    "print(f'accuracy score for training dataset using Linear Regression and L2 loss function : {ORL_model_l2_accuracy_train}')\n",
    "print(f'accuracy score for testing dataset using Linear Regression and L2 loss function : {ORL_model_l2_accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096a518",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Blue </font> is the real target and \n",
    "<font color = 'lime'> Lime </font> is the correct predicted target  and \n",
    "<font color = 'red'> Red </font> is the Misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55455a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_train_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/4, img.shape[1]/4, str(ORL_train_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_train_y[indx-1] != ORL_model_l2_estimate_train[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l2_estimate_train[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l2_estimate_train[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_test_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/5, img.shape[1]/5, str(ORL_test_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_test_y[indx-1] != ORL_model_l2_estimate_test[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l2_estimate_test[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l2_estimate_test[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42775f",
   "metadata": {},
   "source": [
    "#### 2.1.5.2 Linear Regression (L1 loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2dc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on train and test \n",
    "\n",
    "ORL_model_l1_f1_train = f1_score(ORL_train_y, ORL_model_l1_estimate_train, average='macro')\n",
    "ORL_model_l1_f1_test  = f1_score(ORL_test_y, ORL_model_l1_estimate_test, average='macro')\n",
    "\n",
    "# Accuracy score on test\n",
    "\n",
    "ORL_model_l1_accuracy_train = accuracy_score(ORL_train_y, ORL_model_l1_estimate_train)\n",
    "ORL_model_l1_accuracy_test  = accuracy_score(ORL_test_y, ORL_model_l1_estimate_test)\n",
    "\n",
    "# Print Result\n",
    "\n",
    "print(f'f1 score for training dataset using Linear Regression and L2 loss function : {ORL_model_l1_f1_train}')\n",
    "print(f'f1 score for testing dataset using Linear Regression and L2 loss function : {ORL_model_l1_f1_test}')\n",
    "print(f'accuracy score for training dataset using Linear Regression and L2 loss function : {ORL_model_l1_accuracy_train}')\n",
    "print(f'accuracy score for testing dataset using Linear Regression and L2 loss function : {ORL_model_l1_accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5fed43",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Blue </font> is the real target and \n",
    "<font color = 'lime'> Lime </font> is the correct predicted target  and \n",
    "<font color = 'red'> Red </font> is the Misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_train_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/4, img.shape[1]/4, str(ORL_train_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_train_y[indx-1] != ORL_model_l1_estimate_train[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l1_estimate_train[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l1_estimate_train[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_test_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/5, img.shape[1]/5, str(ORL_test_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_test_y[indx-1] != ORL_model_l1_estimate_test[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l1_estimate_test[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_l1_estimate_test[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a947c0",
   "metadata": {},
   "source": [
    "#### 2.1.5.3 OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on train and test \n",
    "\n",
    "ORL_model_omp_f1_train = f1_score(ORL_train_y, ORL_model_omp_estimate_train, average='macro')\n",
    "ORL_model_omp_f1_test  = f1_score(ORL_test_y, ORL_model_omp_estimate_test, average='macro')\n",
    "\n",
    "# Accuracy score on test\n",
    "\n",
    "ORL_model_omp_accuracy_train = accuracy_score(ORL_train_y, ORL_model_omp_estimate_train)\n",
    "ORL_model_omp_accuracy_test  = accuracy_score(ORL_test_y, ORL_model_omp_estimate_test)\n",
    "\n",
    "# Print Result\n",
    "\n",
    "print(f'f1 score for training dataset using Linear Regression and L2 loss function : {ORL_model_omp_f1_train}')\n",
    "print(f'f1 score for testing dataset using Linear Regression and L2 loss function : {ORL_model_omp_f1_test}')\n",
    "print(f'accuracy score for training dataset using Linear Regression and L2 loss function : {ORL_model_omp_accuracy_train}')\n",
    "print(f'accuracy score for testing dataset using Linear Regression and L2 loss function : {ORL_model_omp_accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6e67a",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Blue </font> is the real target and \n",
    "<font color = 'lime'> Lime </font> is the correct predicted target  and \n",
    "<font color = 'red'> Red </font> is the Misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_train_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/4, img.shape[1]/4, str(ORL_train_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_train_y[indx-1] != ORL_model_omp_estimate_train[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_omp_estimate_train[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_omp_estimate_train[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_test_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/5, img.shape[1]/5, str(ORL_test_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_test_y[indx-1] != ORL_model_omp_estimate_test[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_omp_estimate_test[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_model_omp_estimate_test[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c16c2",
   "metadata": {},
   "source": [
    "## 2.2 Method 2\n",
    "\n",
    "In this method, we use train data as a base to reconstruct test data. Simple Steps :\n",
    "\n",
    "1. For each $x_i \\in C_j$, reconstruct test sample $z_k$ :\n",
    "\n",
    "    - that is write $z_k$ as a linear combination of $x_i$s : $\\hat z_k = \\sum_{x_i \\in C_j}\\alpha_i\\times x_i$\n",
    "    \n",
    "2. Calculate  $Loss_j(z_k, \\hat z_k)$.\n",
    "\n",
    "3. Find the minimum $Loss_j$ and its respected $C_j$\n",
    "\n",
    "4. test sample $z_k$ belongs to Class $j$\n",
    "\n",
    "\n",
    "How to build a linear combination ?\n",
    "\n",
    "Assume $X_1, \\dots X_{10}$ are in Class 1.  Our objective is to create the model $f = W_1 X_1 + W_2 X_2 +\\dots + W_{10} X_{10}$ such that $f = \\hat z$, where $\\hat z$ is the estimation of sample test $z$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat z^T = W^TX\\\\\n",
    "W^T = \\begin{bmatrix} W_1,W_2,\\dots, W_{10} \\end{bmatrix} \\hspace{2.2cm} X =\\begin{bmatrix} X_1\\\\X_2\\\\\\vdots\\\\X_{10}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Cosidering $Loss(z , \\hat z)$, we can choose the best class to predict our sample $z$ of test dataset.\n",
    "\n",
    "\n",
    "$\\bullet$ First we need to build $X$ for each Class.\n",
    "\n",
    "$\\bullet$ then we need to build our model one each $X$ for each sample and calculate its $Loss$.\n",
    "\n",
    "$\\bullet$ Predict the class of each sample\n",
    "\n",
    "\n",
    "In General :\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat z = X^T W \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe to split in differenct classes\n",
    "\n",
    "train_dataframe = pd.DataFrame({'x':ORL_train_x.tolist(), 't': ORL_train_y})\n",
    "\n",
    "# Sort by classes\n",
    "\n",
    "train_dataframe = train_dataframe.sort_values(by='t')\n",
    "\n",
    "# Split to different classes\n",
    "\n",
    "ORL_split_train = {}\n",
    "ORL_split_train_keys   = []\n",
    "\n",
    "for c in np.unique(train_dataframe['t']):\n",
    "    ORL_split_train[c] = np.array(train_dataframe[train_dataframe['t'] == c]['x'])\n",
    "    ORL_split_train_keys.append(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afeb649",
   "metadata": {},
   "source": [
    "### 2.2.1 Linear Regression (L2 Loss Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde12ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORL_l2_prediction = []\n",
    "\n",
    "for test_sample in ORL_test_x:\n",
    "    \n",
    "    loss_list = []\n",
    "    loss_keys = []\n",
    "    # key is the class\n",
    "    for key in ORL_split_train_keys:\n",
    "        \n",
    "        # Variables (input and output)\n",
    "        x = np.array([np.array(xi) for xi in ORL_split_train[key]]).T\n",
    "        y = test_sample.reshape(-1,1)\n",
    "        x = Variable(torch.Tensor(x))\n",
    "        y = Variable(torch.Tensor(y))\n",
    "        \n",
    "        # create model\n",
    "        model = LinearRegressionModel(x.shape[1], y.shape[1],criterion_mode='l2',max_iter=100, optimizer_lr=0.01)\n",
    "        \n",
    "        # calculate loss for reconstruction using class 'key'\n",
    "        loss = model.fit(x,y, False)\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        loss_keys.append(key)\n",
    "  \n",
    "    # Predict the label of test sample\n",
    "    \n",
    "    min_loss = np.argmin(loss_list)\n",
    "    label_pred  = loss_keys[min_loss]\n",
    "    \n",
    "    ORL_l2_prediction.append(label_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465fa34",
   "metadata": {},
   "source": [
    "#### Results Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c52fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on test \n",
    "\n",
    "ORL_model2_l2_f1_test  = f1_score(ORL_test_y, ORL_l2_prediction, average='macro')\n",
    "\n",
    "# Accuracy score on test\n",
    "\n",
    "ORL_model2_l2_accuracy_test  = accuracy_score(ORL_test_y, ORL_l2_prediction)\n",
    "\n",
    "# Print Result\n",
    "\n",
    "print(f'f1 score for testing dataset using Linear Regression and L2 loss function : {ORL_model2_l2_f1_test}')\n",
    "print(f'accuracy score for testing dataset using Linear Regression and L2 loss function : {ORL_model2_l2_accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7b8cb",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Blue </font> is the real target and \n",
    "<font color = 'lime'> Lime </font> is the correct predicted target  and \n",
    "<font color = 'red'> Red </font> is the Misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_test_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/5, img.shape[1]/5, str(ORL_test_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_test_y[indx-1] != ORL_l2_prediction[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_l2_prediction[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_l2_prediction[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8651dd",
   "metadata": {},
   "source": [
    "### 2.2.2 Linear Regression (L1 Loss Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORL_l1_prediction = []\n",
    "\n",
    "for test_sample in ORL_test_x:\n",
    "    \n",
    "    loss_list = []\n",
    "    loss_keys = []\n",
    "    # key is the class\n",
    "    for key in ORL_split_train_keys:\n",
    "        \n",
    "        # Variables (input and output)\n",
    "        x = np.array([np.array(xi) for xi in ORL_split_train[key]]).T\n",
    "        y = test_sample.reshape(-1,1)\n",
    "        x = Variable(torch.Tensor(x))\n",
    "        y = Variable(torch.Tensor(y))\n",
    "        \n",
    "        # create model\n",
    "        model = LinearRegressionModel(x.shape[1], y.shape[1],criterion_mode='l1',max_iter=100, optimizer_lr=0.01)\n",
    "        \n",
    "        # calculate loss for reconstruction using class 'key'\n",
    "        loss = model.fit(x,y, False)\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        loss_keys.append(key)\n",
    "  \n",
    "    # Predict the label of test sample\n",
    "    \n",
    "    min_loss = np.argmin(loss_list)\n",
    "    label_pred  = loss_keys[min_loss]\n",
    "    \n",
    "    ORL_l1_prediction.append(label_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f3b0e",
   "metadata": {},
   "source": [
    "#### Results Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03637309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on test \n",
    "\n",
    "ORL_model2_l1_f1_test  = f1_score(ORL_test_y, ORL_l1_prediction, average='macro')\n",
    "\n",
    "# Accuracy score on test\n",
    "\n",
    "ORL_model2_l1_accuracy_test  = accuracy_score(ORL_test_y, ORL_l1_prediction)\n",
    "\n",
    "# Print Result\n",
    "\n",
    "print(f'f1 score for testing dataset using Linear Regression and L2 loss function : {ORL_model2_l1_f1_test}')\n",
    "print(f'accuracy score for testing dataset using Linear Regression and L2 loss function : {ORL_model2_l1_accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d51f31",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Blue </font> is the real target and \n",
    "<font color = 'lime'> Lime </font> is the correct predicted target  and \n",
    "<font color = 'red'> Red </font> is the Misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e17f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_test_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/5, img.shape[1]/5, str(ORL_test_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_test_y[indx-1] != ORL_l1_prediction[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_l1_prediction[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_l1_prediction[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff3838",
   "metadata": {},
   "source": [
    "### 2.2.3 OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORL_omp_prediction = []\n",
    "\n",
    "for test_sample in ORL_test_x:\n",
    "    \n",
    "    loss_list = []\n",
    "    loss_keys = []\n",
    "    # key is the class\n",
    "    for key in ORL_split_train_keys:\n",
    "        \n",
    "        # Variables (input and output)\n",
    "        x = np.array([np.array(xi) for xi in ORL_split_train[key]]).T\n",
    "        y = test_sample.reshape(-1,1)\n",
    "        \n",
    "        # create model\n",
    "        model = OMP(n_nonzero_coefs=2,fit_intercept=False, normalize= False)\n",
    "        \n",
    "        # calculate loss for reconstruction using class 'key'\n",
    "        loss = model.fit(x,y).score(x,y)\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        loss_keys.append(key)\n",
    "  \n",
    "    # Predict the label of test sample\n",
    "    # since Loss is the score, the bigger one is the better one.\n",
    "    \n",
    "    min_loss = np.argmax(loss_list)\n",
    "    label_pred  = loss_keys[min_loss]\n",
    "    \n",
    "    ORL_omp_prediction.append(label_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c42261",
   "metadata": {},
   "source": [
    "#### Results Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b978f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on test \n",
    "\n",
    "ORL_model2_omp_f1_test  = f1_score(ORL_test_y, ORL_omp_prediction, average='macro')\n",
    "\n",
    "# Accuracy score on test\n",
    "\n",
    "ORL_model2_omp_accuracy_test  = accuracy_score(ORL_test_y, ORL_omp_prediction)\n",
    "\n",
    "# Print Result\n",
    "\n",
    "print(f'f1 score for testing dataset using Linear Regression and L2 loss function : {ORL_model2_omp_f1_test}')\n",
    "print(f'accuracy score for testing dataset using Linear Regression and L2 loss function : {ORL_model2_omp_accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b1bb2",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Blue </font> is the real target and \n",
    "<font color = 'lime'> Lime </font> is the correct predicted target  and \n",
    "<font color = 'red'> Red </font> is the Misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "indx = 1\n",
    "for img in ORL_test_x:\n",
    "    img = img.reshape(64,64)\n",
    "    ax = fig.add_subplot(20,20,indx)\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    # add label aka target aka class  [orange]\n",
    "    ax.text(img.shape[0]/5, img.shape[1]/5, str(ORL_test_y[indx-1]), fontsize = 20, color = 'blue', fontweight='bold')\n",
    "    # add predicted label (lime is correct and red is misclassified)\n",
    "    if ORL_test_y[indx-1] != ORL_omp_prediction[indx-1]:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_omp_prediction[indx-1]), fontsize = 20, color = 'red', fontweight='bold', bbox=dict(fill=False, edgecolor='red', linewidth=2))\n",
    "    else:\n",
    "        ax.text(img.shape[0]/2, img.shape[1]/2, str(ORL_omp_prediction[indx-1]), fontsize = 20, color = 'lime', fontweight='bold')\n",
    "        \n",
    "    indx+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ed1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
