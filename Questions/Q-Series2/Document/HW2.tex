\documentclass[a4paper, 12pt]{article}

\usepackage{dblfnote}
\usepackage[perpage]{footmisc}
\usepackage{indentfirst}
\usepackage{framed}
\usepackage{tikz}
\usepackage{listings}[language=Python]
\usepackage{float}
%\usepackage{multicol}


% Custom colors
\usepackage{color}

%\setcounter{secnumdepth}{0}

\usepackage[top=3cm, bottom=3cm, left = 2cm, right = 2cm]{geometry} 
\geometry{a4paper} 
\usepackage{url}
\usepackage{graphicx} 
\usepackage{amsmath,amssymb}  
\usepackage[hidelinks]{hyperref}
\usepackage[labelformat=empty]{caption}
\usepackage{xepersian}
\settextfont{XB Yas.ttf}
\usepackage[utf8]{inputenc}

%\usepackage{xepersian}

\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newcommand\pythonstyle{\lstset{
		language=Python,
		basicstyle=\ttm,
		morekeywords={self},              % Add keywords here
		keywordstyle=\ttb\color{deepblue},
		emph={MyClass,__init__},          % Custom highlighting
		emphstyle=\ttb\color{deepred},    % Custom highlighting style
		stringstyle=\color{deepgreen},
		frame=single,                         % Any extra options here
		showstringspaces=false
}}


% Python environment
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
		\pythonstyle
		\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}




\begin{document}	
\noindent
\begin{minipage}[c]{5cm}
	\baselineskip=.7cm
	\begin{flushright}
		درس : یادگیری ماشین 
		\\
		دانشجو :
		امیرمحمد خرازی
		\\
		شماره دانشجویی :
		40152521002 
		\\
		استاد درس :  
		\href{mrezghi.ir}{دکتر منصور رزقی آهق}
	\end{flushright}
\end{minipage}
\hfill
\begin{minipage}[c]{3cm}
	\begin{center}
		\href{modares.ac.ir}{
			\includegraphics[width=2cm]{logo.png}}
	\end{center}	
\end{minipage}
\\[1mm]
\hrule depth .5mm \relax
\begin{flushright}
	پرسش‌های کلاسی سری دوم
	\hfill
	دانشکده علوم ریاضی ، گروه علوم کامپیوتر، گرایش داده‌کاوی
	\\
	\vspace{5mm}
	گیت‌هاب درس (
	\href{https://github.com/A-M-Kharazi/Machine-Learning-TMU.git}{لینک}
	)
	\hfill
	گیت‌هاب این پرسش (
	\href{https://github.com/A-M-Kharazi/Machine-Learning-TMU/tree/main/Questions/Q-Series2}{لینک}
	)
\end{flushright}

\hrule depth .5mm\relax

%\tableofcontents
%\newpage


\section*{هدف مسئله}
تعدادی نقطه به تصادف در بازه 
$[0,1]$
در نظر گرفته شده است. می‌خواهیم تابع 
$\sin(2\pi x)$
را بازسازی کنیم.

بدین منظور ۵  دیتاست مختلف به ما داده شده است. این دیتاست‌ها هر کدام با هم تفاوتی دارند. 
\begin{itemize}
	\item
	دیتاست اول : ۱۰ نقطه تصادفی روی همان تابع سینوس است
	\item
	دیتاست دوم : ۱۰ نقطه تصادفی روی همان تابع سینوس است که کمی نویز دارند.
	
	\item
	دیتاست سوم : نمونه های تست ما هستند.
	\item
	دیتاست چهارم: ۱۰ نقطه تصادفی روی تابع سینوس به همراه ۲ نقطه دور افتاده .
	\item
	دیتاست پنجم : شامل ۵۰ نقطه مانند دیتاست دوم است.
\end{itemize}

برای هر دیتاست می‌خواهیم چند جمله‌ای رگرسیون را برای درجات 
$M = 1,\dots,9$
تشکیل داده و سپس مدل‌های بدست آمده را (۳۶ مدل را) روی نمونه‌های آزمون بررسی کنیم و نتایج را با توجه به 
$RMSE$
و 
$MAE$
ارزیابی کنیم.

\section*{چند جمله رگرسیون}
در رگرسیون چند جمله، یک تابع پایه داریم که هر $x$ را به بعد دیگری می‌برد. این تابع پایه در این مسئله 
$x^j$
است.  مثلا اگر داشته باشیم 
$x_i$
و بخواهیم چند جمله درجه 
$M$
آن را بسازیم،
 قرار می‌دهیم:
\[
\Phi(X_i) = 
\begin{bmatrix}
	1\\x_i^1\\x_i^2\\\vdots\\x_i^M
\end{bmatrix}
\]
این 
$\Phi$
داده مارا به بعدی 
$M+1$
برد. یعنی بجایی اینکه از روش های غیر خطی استفاده کنیم، فرض میکنیم داده ما در بعد 
$M+1$
قرار دارد و سپس با روش‌های خطی آن را حل می‌کنیم.

یک جند جمله درحه 
$M$
روی متغیر تک بعدی 
$X$
به شکل زیر است:
\[
W_0  + W_1X_1 + W_2X_1^2 + \dots + W_MX_1^M
\]

واضح است که اگر برداری به 
$X$
ها و 
$Y$
ها نگاه کنیم، برای 
$N$
داده خواهیم داشت 

$Y \approx \Phi X$
که 
\[
\Phi = 
\begin{bmatrix}
	\Phi^T(X_1)\\
	\Phi^T(X_2)\\
	\vdots\\
	\Phi^T(X_N)
\end{bmatrix}
\]

\section*{روش کار}
میخواهیم تفاوت این 
$y$
و 
تخمین زده شده‌ی آن یعنی 
$\Phi W$
حداقل شود. 

می‌توانیم از نرم ۱ یا ۲ برای این کار استفاده کنیم.

ما در اینجا با نرم ۲ می‌رویم. نرم ۲ توان ۲ فاصله‌ها را در نظر می‌گیرد. مشکل اینجاست که  توان ۲ فاصله ها به نویز و نقاط پرت حساس می‌شود. 

\[
\min_W\{||\Phi W - Y\||_2^2\}
\] 
\section*{بهینه سازی}
با کمک نرم ۲ می‌توانیم از روش‌های ماتریسی برای حل این مسئله بهینه سازی استفاده کنیم لذا جواب می‌شود :
\[
W = (\Phi^T\Phi)^{-1}\Phi^T y = \Phi^\dagger y
\]

\section*{کد}
با کمک این فرمول‌ها می‌توانیم برای وزن تمامی این ۳۶ مدل تصمیم بگیریم. عدد حالت 
$\Phi^T\Phi$
می‌تواند روی تاثیر نویز بر $W$ واقعی تاثیر داشته باشد. بدین صورت که اگر بد حالت باشد، نویز بیشتری را در نظر می‌گیرد و لذا مدل ما به احتمال زیاد روی داده‌های آزمون بیش‌برازش خواهد داشت.البته در کد ما از 
\lr{scikit-learn}
استفاده می‌کنیم.

\section*{نتیجه}

جزئیات کامل کد و نتیجه گیری‌های گرفته شده به همراه تصاویر در فایل 
\lr{\pythoninline{.Ipynb}}
موجود است. لذا برای بررسی کد می‌توانید به لینک گیت‌هاب، یا گوگل مراجعه کنید. این کد در کنار این گزارش نیز ارسال می‌شود.

در حالت کلی مدل های دیتاست های ۴ و ۲ بد عمل می‌کردند. دیتاست ۵ با داده‌های زیاد مشکل نویزی خود را تا حدی حل کرد، دیتاست اول که نسبتا خیلی خوب بود.
\end{document}


