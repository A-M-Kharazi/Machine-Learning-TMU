{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376044ae",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "476c2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4728f",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4e662ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS = load_iris()\n",
    "\n",
    "data = IRIS['data']\n",
    "labels = IRIS['target']\n",
    "\n",
    "feature_names = IRIS['feature_names']\n",
    "target_names  = IRIS['target_names']\n",
    "\n",
    "dataset = pd.DataFrame({feature_names[0]:data[:,0],\n",
    "                        feature_names[1]:data[:,1],\n",
    "                        feature_names[2]:data[:,2],\n",
    "                        feature_names[3]:data[:,3],\n",
    "                        'type': target_names[labels]})\n",
    "\n",
    "#print(IRIS['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114d532",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6fe671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test, y_train, y_test=train_test_split(data,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74e5a9",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "33ccd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f8592392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1d82177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions \n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "785c65c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c57a0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a3dd32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding  for labels\n",
    "\n",
    "y_train_encoded = OneHotEncoder().fit_transform(y_train.reshape(-1,1))\n",
    "\n",
    "# reshape x_train and x_test so that each sample is transposed\n",
    "\n",
    "x_train_encoded = x_train.reshape(x_train.shape[0] ,1 , x_train.shape[1])\n",
    "x_test_encoded = x_test.reshape(x_test.shape[0] ,1 , x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c7d429bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.200398\n",
      "epoch 2/35   error=0.119874\n",
      "epoch 3/35   error=0.115739\n",
      "epoch 4/35   error=0.112972\n",
      "epoch 5/35   error=0.108534\n",
      "epoch 6/35   error=0.105060\n",
      "epoch 7/35   error=0.097884\n",
      "epoch 8/35   error=0.090345\n",
      "epoch 9/35   error=0.095897\n",
      "epoch 10/35   error=0.083151\n",
      "epoch 11/35   error=0.084270\n",
      "epoch 12/35   error=0.087546\n",
      "epoch 13/35   error=0.088629\n",
      "epoch 14/35   error=0.073346\n",
      "epoch 15/35   error=0.057947\n",
      "epoch 16/35   error=0.075914\n",
      "epoch 17/35   error=0.056034\n",
      "epoch 18/35   error=0.081103\n",
      "epoch 19/35   error=0.083187\n",
      "epoch 20/35   error=0.062784\n",
      "epoch 21/35   error=0.109191\n",
      "epoch 22/35   error=0.077263\n",
      "epoch 23/35   error=0.071554\n",
      "epoch 24/35   error=0.079289\n",
      "epoch 25/35   error=0.082914\n",
      "epoch 26/35   error=0.075938\n",
      "epoch 27/35   error=0.075040\n",
      "epoch 28/35   error=0.078888\n",
      "epoch 29/35   error=0.076598\n",
      "epoch 30/35   error=0.084942\n",
      "epoch 31/35   error=0.072208\n",
      "epoch 32/35   error=0.069539\n",
      "epoch 33/35   error=0.069491\n",
      "epoch 34/35   error=0.073091\n",
      "epoch 35/35   error=0.072457\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "net.add(FCLayer(4, 5))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(5, 10))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(10, 6))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(6, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train_encoded, y_train_encoded.toarray(), epochs=35, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a5cb09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = net.predict(x_train_encoded)\n",
    "out_test  = net.predict(x_test_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9142e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score : 0.9416666666666667\n",
      "Test F1-Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "out_train_labels_MLP = []\n",
    "out_test_labels_MLP  = []\n",
    "\n",
    "for output in out_train:\n",
    "    out_train_labels_MLP.append(np.argmax(output[0]))\n",
    "    \n",
    "for output in out_test:\n",
    "    out_test_labels_MLP.append(np.argmax(output[0]))\n",
    "    \n",
    "train_accuracy_score = accuracy_score(y_train,out_train_labels_MLP)\n",
    "test_accuracy_score  = accuracy_score(y_test,out_test_labels_MLP)\n",
    "\n",
    "print(f'Train accuracy_score : {train_accuracy_score}\\nTest accuracy_score : {test_accuracy_score}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c19ad",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bbfd2e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy_score : 0.9583333333333334\n",
      "Test accuracy_score : 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "SVM.fit(x_train,y_train)\n",
    "\n",
    "out_train_labels_SVM = SVM.predict(x_train) \n",
    "out_test_labels_SVM  = SVM.predict(x_test)\n",
    "\n",
    "train_accuracy_score = accuracy_score(y_train,out_train_labels_SVM)\n",
    "test_accuracy_score  = accuracy_score(y_test,out_test_labels_SVM)\n",
    "\n",
    "print(f'Train accuracy_score : {train_accuracy_score}\\nTest accuracy_score : {test_accuracy_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91de65",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "https://betterdatascience.com/mml-decision-trees/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1f999220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    Helper class which implements a single tree node.\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b383607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    '''\n",
    "    Class which implements a decision tree classifier algorithm.\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _entropy(s):\n",
    "        '''\n",
    "        Helper function, calculates entropy from an array of integer values.\n",
    "        \n",
    "        :param s: list\n",
    "        :return: float, entropy value\n",
    "        '''\n",
    "        # Convert to integers to avoid runtime errors\n",
    "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
    "        # Probabilities of each class label\n",
    "        percentages = counts / len(s)\n",
    "\n",
    "        # Caclulate entropy\n",
    "        entropy = 0\n",
    "        for pct in percentages:\n",
    "            if pct > 0:\n",
    "                entropy += pct * np.log2(pct)\n",
    "        return -entropy\n",
    "    \n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        '''\n",
    "        Helper function, calculates information gain from a parent and two child nodes.\n",
    "        \n",
    "        :param parent: list, the parent node\n",
    "        :param left_child: list, left child of a parent\n",
    "        :param right_child: list, right child of a parent\n",
    "        :return: float, information gain\n",
    "        '''\n",
    "        num_left = len(left_child) / len(parent)\n",
    "        num_right = len(right_child) / len(parent)\n",
    "        \n",
    "        # One-liner which implements the previously discussed formula\n",
    "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        '''\n",
    "        Helper function, calculates the best split for given features and target\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: dict\n",
    "        '''\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # For every dataset feature\n",
    "        for f_idx in range(n_cols):\n",
    "            X_curr = X[:, f_idx]\n",
    "            # For every unique value of that feature\n",
    "            for threshold in np.unique(X_curr):\n",
    "                # Construct a dataset and split it to the left and right parts\n",
    "                # Left part includes records lower or equal to the threshold\n",
    "                # Right part includes records higher than the threshold\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "\n",
    "                # Do the calculation only if there's data in both subsets\n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "                    # Obtain the value of the target variable for subsets\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "\n",
    "                    # Caclulate the information gain and save the split parameters\n",
    "                    # if the current split if better then the previous best\n",
    "                    gain = self._information_gain(y, y_left, y_right)\n",
    "                    if gain > best_info_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "        return best_split\n",
    "    \n",
    "    def _build(self, X, y, depth=0):\n",
    "        '''\n",
    "        Helper recursive function, used to build a decision tree from the input data.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :param depth: current depth of a tree, used as a stopping criteria\n",
    "        :return: Node\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # Check to see if a node should be leaf node\n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best = self._best_split(X, y)\n",
    "            # If the split isn't pure\n",
    "            if best['gain'] > 0:\n",
    "                # Build a tree on the left\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1], \n",
    "                    y=best['df_left'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1], \n",
    "                    y=best['df_right'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'], \n",
    "                    threshold=best['threshold'], \n",
    "                    data_left=left, \n",
    "                    data_right=right, \n",
    "                    gain=best['gain']\n",
    "                )\n",
    "        # Leaf node - value is the most common target value \n",
    "        return Node(\n",
    "            value=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Function used to train a decision tree classifier model.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: None\n",
    "        '''\n",
    "        # Call a recursive function to build the tree\n",
    "        self.root = self._build(X, y)\n",
    "        \n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Helper recursive function, used to predict a single instance (tree traversal).\n",
    "        \n",
    "        :param x: single observation\n",
    "        :param tree: built tree\n",
    "        :return: float, predicted class\n",
    "        '''\n",
    "        # Leaf node\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        # Go to the left\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_left)\n",
    "        \n",
    "        # Go to the right\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_right)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function used to classify new instances.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :return: np.array, predicted classes\n",
    "        '''\n",
    "        # Call the _predict() function for every observation\n",
    "        return [self._predict(x, self.root) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "bc3cb34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy_score : 1.0\n",
      "Test accuracy_score : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTree()\n",
    "DT.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "out_train_labels_DT = DT.predict(x_train) \n",
    "out_test_labels_DT  = DT.predict(x_test)\n",
    "\n",
    "train_accuracy_score = accuracy_score(y_train,out_train_labels_DT)\n",
    "test_accuracy_score  = accuracy_score(y_test,out_test_labels_DT)\n",
    "\n",
    "print(f'Train accuracy_score : {train_accuracy_score}\\nTest accuracy_score : {test_accuracy_score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
